{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SportsMonks Predictive Analysis\n",
    "\n",
    "Using the data gathered through the [Sports Monks API](https://www.sportmonks.com/), we can begin to analyze the match and player data and try to predict match outcomes. We can do this with the [SciKit-Learn library](http://scikit-learn.org/stable/), which provides a collection of machine learning models that can be tuned to the specific problem.\n",
    "\n",
    "Specifically, we're going to be using the [Supervised Neural Networks Classifier](http://scikit-learn.org/stable/modules/neural_networks_supervised.html) in the SciKit-Learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "First, we need to set up access to the local SportsMonks database, as well as importing [Pandas](http://pandas.pydata.org/pandas-docs/version/0.23/) and [Numpy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "sys.path.insert(0, './internal')\n",
    "import databaseinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('mysql+mysqlconnector://{}:{}@{}/{}'.format(\n",
    "    databaseinfo.db_user(),\n",
    "    databaseinfo.db_passwd(),\n",
    "    databaseinfo.db_host(),\n",
    "    databaseinfo.db_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to retrieve and merge the Club Season data and the Starting Lineup data for each match, for both the home and away teams. After this is done, we can use this data to train the Neural Network and determine the optimal number of hidden layers and nodes that gives us the most accurate match prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toSqlRename(tableName, attribute, prefix):\n",
    "    return \"%s.%s as %s%s\" % (tableName, attribute, prefix, attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_attributes = [\n",
    "    \"minutes_played\",\n",
    "    \"appearances\",\n",
    "    \"goals\",\n",
    "    \"goals_conceded\",\n",
    "    \"assists\",\n",
    "    \"shots_on_goal\",\n",
    "    \"shots_total\",\n",
    "    \"fouls_committed\",\n",
    "    \"fouls_drawn\",\n",
    "    \"interceptions\",\n",
    "    \"saves\",\n",
    "    \"clearances\",\n",
    "    \"tackles\",\n",
    "    \"offsides\",\n",
    "    \"blocks\",\n",
    "    \"pen_saved\",\n",
    "    \"pen_missed\",\n",
    "    \"pen_scored\",\n",
    "    \"passes_total\",\n",
    "    \"crosses_total\"\n",
    "]\n",
    "\n",
    "def player_rename(tableName, attribute, prefix):\n",
    "    return \"%s.%s as %s%s\" % (tableName, attribute, prefix, attribute)\n",
    "\n",
    "def home_player_rename(attribute):\n",
    "    return player_rename(\"ls\", attribute, \"home_\")\n",
    "\n",
    "def away_player_rename(attribute):\n",
    "    return player_rename(\"ls\", attribute, \"away_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homeRename(attribute):\n",
    "    return toSqlRename(\"css\", attribute, \"home_\")\n",
    "\n",
    "def awayRename(attribute):\n",
    "    return toSqlRename(\"css\", attribute, \"away_\")\n",
    "\n",
    "attributes = [\n",
    "    \"win_total\",\n",
    "    \"draw_total\",\n",
    "    \"lost_total\",\n",
    "    \"goals_for_total\",\n",
    "    \"goals_against_total\",\n",
    "    \"clean_sheet_total\",\n",
    "    \"failed_to_score_total\"\n",
    "]\n",
    "\n",
    "home_string = \", \".join(list(map(homeRename, attributes)))\n",
    "home_players_string = \", \".join(list(map(home_player_rename, player_attributes)))\n",
    "away_string = \", \".join(list(map(awayRename, attributes)))\n",
    "away_players_string = \", \".join(list(map(away_player_rename, player_attributes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"SELECT * \\\n",
    "        FROM Fixture f, ClubSeasonStats css, LineupStats ls \\\n",
    "        WHERE f.season_id=css.season_id \\\n",
    "            AND f.home_team_id=css.club_id \\\n",
    "            AND f.id=ls.fixture_id \\\n",
    "            AND f.home_team_id=ls.club_id\"\n",
    "\n",
    "club_attribute_query = \"SELECT home.*, %s, %s \\\n",
    "FROM (  SELECT f.*, %s, %s \\\n",
    "        FROM Fixture f, ClubSeasonStats css, LineupStats ls \\\n",
    "        WHERE f.season_id=css.season_id \\\n",
    "            AND f.home_team_id=css.club_id \\\n",
    "            AND f.id=ls.fixture_id \\\n",
    "            AND f.home_team_id=ls.club_id \\\n",
    "     ) home, \\\n",
    "    ClubSeasonStats css, \\\n",
    "    LineupStats ls \\\n",
    "WHERE home.season_id=css.season_id \\\n",
    "    AND home.away_team_id=css.club_id  \\\n",
    "    AND home.id=ls.fixture_id \\\n",
    "    AND home.away_team_id=ls.club_id\" % (away_string, away_players_string, home_string, home_players_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resoverall = engine.execute(club_attribute_query)\n",
    "df = pd.DataFrame(resoverall.fetchall())\n",
    "df.columns = resoverall.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Now that we have the data, we need to do something with it. Initially, let's just look at the overall result of a game, giving the three outcomes a corresponding label:\n",
    "* Home team wins: 'H'\n",
    "* Teams draw: 'D'\n",
    "* Away team wins: 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResult(scores):\n",
    "    home_score = scores[0]\n",
    "    away_score = scores[1]\n",
    "    \n",
    "    if home_score > away_score:\n",
    "        return 'H'\n",
    "    elif home_score == away_score:\n",
    "        return 'D'\n",
    "    else:\n",
    "        return 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = df.loc[:, ['home_team_score', 'away_team_score']]\n",
    "df['Result'] = scores.apply(getResult, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After assigning these labels, we extract the input and output values and split them into the training data and the test data (80% train, 20% test). We can use Python's random library to accomplish this, combined with the array capabilities of Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "X = np.array([list(x) for x in df.loc[:, 'home_win_total':'away_crosses_total'].values])\n",
    "Y = np.array(df['Result'].values)\n",
    "\n",
    "train_range = range(0, len(X), 1)\n",
    "train = random.sample(train_range, int(math.floor(len(train_range) * 0.8)))\n",
    "test = [x for x in train_range if x not in train]\n",
    "\n",
    "X_train = X[train]\n",
    "Y_train = Y[train]\n",
    "\n",
    "X_test = X[test]\n",
    "Y_test = Y[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the training and the test data, we can use SciKit-Learn's Pipeline and cross-validation-score imports in order to test a series of models. For these models, we can test the number of hidden layers and the number of nodes in each hidden layer.\n",
    "\n",
    "Using the formula in Section 4.2 of a [Neural Networks paper](https://tinyurl.com/ybhoz5ea), I was able to estimate the number of optimal hidden layers and perform a smaller analysis. I also used the guidance on [this StackExchange post](https://tinyurl.com/ydhcc39y) to limit the number of hidden layers to either a single layer or two layers. Three or more hidden layers require an extremely large dataset to draw from, as well as computing power that isn't available to me.\n",
    "\n",
    "*Note: These tests take a long time to run. The analysis I ran is summarized after this section, and the code is left here for re-use, if necessary.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_score(n1, n2):\n",
    "    if n2 > 0:\n",
    "        hidden_layers = (n1, n2)\n",
    "    else:\n",
    "        hidden_layers = (n1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    model = MLPClassifier(max_iter=500, hidden_layer_sizes=hidden_layers)\n",
    "    pipeline = Pipeline([('transform', scaler), ('fit', model)])\n",
    "    return cross_val_score(pipeline, X_train, Y_train, cv=10, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "cv_errors = []\n",
    "\n",
    "for n1 in range(1, 15, 1):\n",
    "    for n2 in range(0, 15, 1):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            nums = ((n1, n2))\n",
    "            cv_error = get_cv_error(n1, n2)\n",
    "            print((nums, cv_error))\n",
    "            cv_errors.append((nums, cv_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overview of this analysis can be seen below. Any unmentioned combination of *(n1, n2)* had a lower cross-validation score than the below combinations:\n",
    "* Single hidden layer with n1=12 = **0.5917**\n",
    "* Two layers with n1=12 and n2=11 = **0.5922**\n",
    "* Two layers with n1=12 and n2=14 = **0.5933**\n",
    "* Two layers with n1=10 and n2=10 = **0.5943**\n",
    "* Two layers with n1=11 and n2=13 = **0.5950**\n",
    "\n",
    "Most of the other combinations landed between 0.53 and 0.58, with a gradual increase until the above values, followed by a small decrease in cross-validation score, and finally a plateau (usually around 0.578)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the Test Dataset\n",
    "\n",
    "Awesome, now we have the \"optimal\" hidden layer values for the given dataset. We can then fit the training dataset to the optimal model and predict the match results of the test dataset. Then, using some more tools from SciKit-Learn, we can view the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) of the predictions, as well as the precision, recall, and [F1-score](https://en.wikipedia.org/wiki/F1_score) of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "            \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    model = MLPClassifier(max_iter=500, hidden_layer_sizes=(11, 13))\n",
    "\n",
    "    X_train_std = scaler.transform(X_train)\n",
    "    model.fit(X_train_std, Y_train)\n",
    "\n",
    "    X_test_std = scaler.transform(X_test)\n",
    "    predictions = model.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644\n",
      "[[730  77 135]\n",
      " [228 151 137]\n",
      " [216  65 363]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(Y_test, predictions, labels=['H', 'D', 'A']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          A       0.57      0.56      0.57       644\n",
      "          D       0.52      0.29      0.37       516\n",
      "          H       0.62      0.77      0.69       942\n",
      "\n",
      "avg / total       0.58      0.59      0.57      2102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(Y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret these:\n",
    "* Correctly predicted (top-left to bottom-right diagonal):\n",
    "  * 730 Home wins\n",
    "  * 151 Draws\n",
    "  * 363 Away wins.\n",
    "  * 1244 correct out of 2102 = **59.2% correct**\n",
    "* Incorrectly predicted:\n",
    "  * Predicted Home wins that were wrong:\n",
    "    * 228 Draws.\n",
    "    * 216 Away wins.\n",
    "  * Predicted Draws that were wrong:\n",
    "    * 77 Home wins.\n",
    "    * 65 Away wins.\n",
    "  * Predicted Away wins that were wrong:\n",
    "    * 135 Home wins.\n",
    "    * 137 Draws.\n",
    "\n",
    "We can see that the Neural Network heavily leans towards Home wins, which backs up the theory that [Home Field Advantage in soccer is huge](http://freakonomics.com/2011/12/18/football-freakonomics-how-advantageous-is-home-field-advantage-and-why/). We can see the results of this in the recall score: Home wins have a 77% chance of being correctly recalled, while draws only have a *29%* chance of being recalled. This huge predictive imbalance between home wins and draws is split almost directly in the middle by correctly predicted away wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curiosity About Player Statistics\n",
    "\n",
    "After doing the above analysis, which uses both the team's statistics and the starting lineups statistics, I was curious about how much the model would be impacted if I *only* factored in the statistics for starting lineups. This analysis can be seen below, but for a quick overview:\n",
    "\n",
    "* The cross-validation score was, on average, about 0.02 points lower than the model that uses the team's record.\n",
    "* Home wins were favored even more heavily than in the above model, leading to a **0.01** recall rate for Draws.\n",
    "* Classification Report average scores were about 0.05 lower than the above model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_home = df.loc[:, 'home_goals':'home_crosses_total']\n",
    "X_away = df.loc[:, 'away_goals':'away_crosses_total']\n",
    "X = np.array([list(x) for x in pd.merge(X_home, X_away, left_index=True, right_index=True).values])\n",
    "Y = np.array(df['Result'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = range(0, len(X), 1)\n",
    "train = random.sample(train_range, int(math.floor(len(train_range) * 0.8)))\n",
    "test = [x for x in train_range if x not in train]\n",
    "\n",
    "X_train = X[train]\n",
    "Y_train = Y[train]\n",
    "\n",
    "X_test = X[test]\n",
    "Y_test = Y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 7)\n",
      "0.5665457312060033\n",
      "(5, 8)\n",
      "0.563098300979191\n",
      "(5, 9)\n",
      "0.5639330498633124\n",
      "(5, 10)\n",
      "0.5678551222244337\n",
      "(5, 11)\n",
      "0.5664286670442823\n",
      "(5, 12)\n",
      "0.567854983024441\n",
      "(6, 7)\n",
      "0.5657186215866863\n",
      "(6, 8)\n",
      "0.5685668521965888\n",
      "(6, 9)\n",
      "0.5690457389574373\n",
      "(6, 10)\n",
      "0.5626208395343661\n",
      "(6, 11)\n",
      "0.5645291389735635\n",
      "(6, 12)\n",
      "0.5660705363317885\n",
      "(7, 7)\n",
      "0.564288360585607\n",
      "(7, 8)\n",
      "0.5652335175135071\n",
      "(7, 9)\n",
      "0.5692787348421724\n",
      "(7, 10)\n",
      "0.5659554448534541\n",
      "(7, 11)\n",
      "0.562032247793037\n",
      "(7, 12)\n",
      "0.5671419632232455\n",
      "(8, 7)\n",
      "0.5649999564205272\n",
      "(8, 8)\n",
      "0.5652438523856909\n",
      "(8, 9)\n",
      "0.5634540282879504\n",
      "(8, 10)\n",
      "0.5660730819652352\n",
      "(8, 11)\n",
      "0.5629813780348712\n",
      "(8, 12)\n",
      "0.5663132927901016\n",
      "(9, 7)\n",
      "0.5640487001590261\n",
      "(9, 8)\n",
      "0.5665482738133372\n",
      "(9, 9)\n",
      "0.5646360168896899\n",
      "(9, 10)\n",
      "0.5629785489531086\n",
      "(9, 11)\n",
      "0.5670241831996642\n",
      "(9, 12)\n",
      "0.5629771307040266\n",
      "(10, 7)\n",
      "0.5621450686659439\n",
      "(10, 8)\n",
      "0.5638118853149593\n",
      "(10, 9)\n",
      "0.5670231977075748\n",
      "(10, 10)\n",
      "0.5665523893509724\n",
      "(10, 11)\n",
      "0.5678562523022833\n",
      "(10, 12)\n",
      "0.5622665233993314\n",
      "(11, 7)\n",
      "0.5651182922161386\n",
      "(11, 8)\n",
      "0.5638098964730673\n",
      "(11, 9)\n",
      "0.5623806162531839\n",
      "(11, 10)\n",
      "0.5613173938383825\n",
      "(11, 11)\n",
      "0.561311455927528\n",
      "(11, 12)\n",
      "0.5620247487405367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((5, 7), 0.5665457312060033),\n",
       " ((5, 8), 0.563098300979191),\n",
       " ((5, 9), 0.5639330498633124),\n",
       " ((5, 10), 0.5678551222244337),\n",
       " ((5, 11), 0.5664286670442823),\n",
       " ((5, 12), 0.567854983024441),\n",
       " ((6, 7), 0.5657186215866863),\n",
       " ((6, 8), 0.5685668521965888),\n",
       " ((6, 9), 0.5690457389574373),\n",
       " ((6, 10), 0.5626208395343661),\n",
       " ((6, 11), 0.5645291389735635),\n",
       " ((6, 12), 0.5660705363317885),\n",
       " ((7, 7), 0.564288360585607),\n",
       " ((7, 8), 0.5652335175135071),\n",
       " ((7, 9), 0.5692787348421724),\n",
       " ((7, 10), 0.5659554448534541),\n",
       " ((7, 11), 0.562032247793037),\n",
       " ((7, 12), 0.5671419632232455),\n",
       " ((8, 7), 0.5649999564205272),\n",
       " ((8, 8), 0.5652438523856909),\n",
       " ((8, 9), 0.5634540282879504),\n",
       " ((8, 10), 0.5660730819652352),\n",
       " ((8, 11), 0.5629813780348712),\n",
       " ((8, 12), 0.5663132927901016),\n",
       " ((9, 7), 0.5640487001590261),\n",
       " ((9, 8), 0.5665482738133372),\n",
       " ((9, 9), 0.5646360168896899),\n",
       " ((9, 10), 0.5629785489531086),\n",
       " ((9, 11), 0.5670241831996642),\n",
       " ((9, 12), 0.5629771307040266),\n",
       " ((10, 7), 0.5621450686659439),\n",
       " ((10, 8), 0.5638118853149593),\n",
       " ((10, 9), 0.5670231977075748),\n",
       " ((10, 10), 0.5665523893509724),\n",
       " ((10, 11), 0.5678562523022833),\n",
       " ((10, 12), 0.5622665233993314),\n",
       " ((11, 7), 0.5651182922161386),\n",
       " ((11, 8), 0.5638098964730673),\n",
       " ((11, 9), 0.5623806162531839),\n",
       " ((11, 10), 0.5613173938383825),\n",
       " ((11, 11), 0.561311455927528),\n",
       " ((11, 12), 0.5620247487405367)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_errors = []\n",
    "\n",
    "# Use slightly different ranges, as we now have a lower number of input values\n",
    "for n1 in range(5, 12, 1):\n",
    "    for n2 in range(7, 13, 1):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            nums = ((n1, n2))\n",
    "            cv_error = get_cv_error(n1, n2)\n",
    "            print(nums)\n",
    "            print(cv_error)\n",
    "            cv_errors.append((nums, cv_error))\n",
    "            \n",
    "cv_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "model = MLPClassifier(max_iter=500, hidden_layer_sizes=(7, 9))\n",
    "\n",
    "X_train_std = scaler.transform(X_train)\n",
    "model.fit(X_train_std, Y_train)\n",
    "\n",
    "X_test_std = scaler.transform(X_test)\n",
    "predictions = model.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[781   4 147]\n",
      " [341   6 193]\n",
      " [245   2 383]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          A       0.53      0.61      0.57       630\n",
      "          D       0.50      0.01      0.02       540\n",
      "          H       0.57      0.84      0.68       932\n",
      "\n",
      "avg / total       0.54      0.56      0.48      2102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_test, predictions, labels=['H', 'D', 'A']))\n",
    "print(classification_report(Y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
